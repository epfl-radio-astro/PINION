{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee52fe0",
   "metadata": {},
   "source": [
    "# *Low Data* training\n",
    "This script produces the training with the *low data* settings. This settings trains the network with two losses: the ODE constraint and the data loss. However, for the data loss, only a limited sample of snapshots are picked for the training. This possibly reduces the data bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66e788-b588-48b4-98c5-060e838aabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of snapshots to keep\n",
    "cut = slice(0, 46, 10)\n",
    "\n",
    "# Selects the root of the project\n",
    "project_root = \"/path/to/project/root\"\n",
    "\n",
    "# files location\n",
    "filepath = '/path/to/simulation/files/folder/' # macos\n",
    "savepath = '/path/to/results/folder/'\n",
    "memmap = True\n",
    "\n",
    "\n",
    "# training settings\n",
    "from calendar import different_locale\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Keeps track of the current config.\"\"\"\n",
    "    kernel_size: int = 3\n",
    "    n_pool: int = 2\n",
    "    nb_train: int = 1000\n",
    "    nb_test: int = 10\n",
    "    subvolume_size: int = 19\n",
    "    batch_size: int = 4600//5\n",
    "    show: bool = False\n",
    "    nb_epoch: int = 400\n",
    "    plot_size: int = 50\n",
    "    pinn_multiplication_factor: float = 1.0\n",
    "    fcn_div_factor: int = 2\n",
    "    n_fcn_layers: int = 5\n",
    "    n_features: int = 64\n",
    "    score: str = 'mse' # other choice: r2\n",
    "    maxpool_size: int = 2\n",
    "    maxpool_stride: int = 2\n",
    "    \n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"Run configuration:\n",
    "--------------------------------\n",
    "kernel_size: {self.kernel_size}\n",
    "n_pool: {self.n_pool}\n",
    "nb_train: {self.nb_train}\n",
    "nb_test: {self.nb_test}\n",
    "subvolume_size: {self.subvolume_size}\n",
    "batch_size: {self.batch_size}\n",
    "show: {self.show}\n",
    "nb_epoch: {self.nb_epoch}\n",
    "plot_size: {self.plot_size}\n",
    "pinn_multiplication_factor: {self.pinn_multiplication_factor}\n",
    "fcn_div_factor: {self.fcn_div_factor}\n",
    "n_fcn_layers: {self.n_fcn_layers}\n",
    "n_features: {self.n_features}\n",
    "score: {self.score}\n",
    "maxpool_size: {self.maxpool_size}\n",
    "maxpool_stride: {self.maxpool_stride}\n",
    "--------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Choice of configuration\n",
    "myconfig = Config(kernel_size=3, n_pool=3, subvolume_size=7, n_features=64, score='r2', maxpool_stride=1, nb_train=20000, nb_test=500, batch_size=4600//5*20, fcn_div_factor=4, n_fcn_layers=5, show=True)\n",
    "\n",
    "kernel_size = myconfig.kernel_size\n",
    "n_pool = myconfig.n_pool\n",
    "nb_train = myconfig.nb_train\n",
    "nb_test = myconfig.nb_test\n",
    "subvolume_size = myconfig.subvolume_size # MUST BE ODD\n",
    "batch_size = myconfig.batch_size # must be a multiple of 46 if PINN is enabled.\n",
    "show = myconfig.show # show the plots or not\n",
    "nb_epoch = myconfig.nb_epoch\n",
    "plot_size = myconfig.plot_size\n",
    "pinn_multiplication_factor = myconfig.pinn_multiplication_factor\n",
    "fcn_div_factor = myconfig.fcn_div_factor\n",
    "n_fcn_layers = myconfig.n_fcn_layers\n",
    "n_features = myconfig.n_features\n",
    "score_type = myconfig.score\n",
    "maxpool_size = myconfig.maxpool_size\n",
    "maxpool_stride = myconfig.maxpool_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafeeb7-762f-437c-8782-e6366b1f9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(project_root)\n",
    "print(\"working from: \" + os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from astropy.cosmology import WMAP3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import glob\n",
    "import collections\n",
    "import astropy.units as u\n",
    "import tools21cm as t2c\n",
    "import tools\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# load the cosmology\n",
    "cosmo = WMAP3\n",
    "\n",
    "# prepare for cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a65824",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb988d-3e5a-4ad8-b0e5-6842e41819da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the files\n",
    "redshifts_str, files_irate  = tools._get_files(filepath, 'irate')\n",
    "redshifts_str, files_xHII   = tools._get_files(filepath, 'xHII')\n",
    "redshifts_str, files_rho   = tools._get_files(filepath, 'rho')\n",
    "redshifts_str, files_nsrc   = tools._get_files(filepath, 'nsrc')\n",
    "redshifts_str, files_mask  = tools._get_files(filepath, 'mask') \n",
    "\n",
    "# load the data\n",
    "redshifts_arr, irates_arr      = tools.load(files_irate, memmap) # 1/s\n",
    "redshifts_arr, xHII_arr        = tools.load(files_xHII, memmap) # unitless\n",
    "redshifts_arr, overdensity_arr = tools.load(files_rho, memmap) # unitless\n",
    "redshifts_arr, nsrc_arr        = tools.load(files_nsrc, memmap) # unitless\n",
    "redshifts_arr, mask_arr        = tools.load(files_mask, memmap) # unitless\n",
    "\n",
    "# apply units\n",
    "irates_arr /= u.s\n",
    "xHII_arr *= (u.m/u.m)\n",
    "overdensity_arr *= (u.m/u.m)\n",
    "redshifts_arr *= (u.m/u.m)\n",
    "nsrc_arr *= (u.m/u.m)\n",
    "mask_arr *= (u.m/u.m)\n",
    "\n",
    "# Now we can convert all the data into their correct form.\n",
    "Om0 = cosmo.Om0\n",
    "alpha = 2.59e-13 * (u.cm**3/u.s) # 2.59e-13 cm^3/s\n",
    "rhoc0 = cosmo.critical_density0 # g/cm3\n",
    "mu = 1.32 * (u.m/u.m) # unitless\n",
    "mp = 1.67e-24 * u.g #1.67e-27 * u.kg # kg\n",
    "\n",
    "# we want to get the n_H information, which is the number density of hydrogen.\n",
    "mu_He = 0.074 * (u.m/u.m)\n",
    "nh_bar = (1-mu_He) * (cosmo.Ob0 * rhoc0)/(mu * mp)\n",
    "nh_bar.to(1/u.cm**3)\n",
    "nh_arr = nh_bar * (1 + overdensity_arr)\n",
    "\n",
    "# get the density from overdensity\n",
    "rho_arr = rhoc0 * (1 + overdensity_arr)\n",
    "\n",
    "del overdensity_arr\n",
    "\n",
    "# load the cosmology and convert redshift to time\n",
    "time_arr = np.asarray([cosmo.age(z).to(u.s).value for z in redshifts_arr], dtype=np.float32) * u.s\n",
    "time_max = np.max(time_arr)\n",
    "norm_time_arr = time_arr / time_max\n",
    "\n",
    "nsrc_max = np.max(nsrc_arr)\n",
    "rho_max = np.max(rho_arr)\n",
    "mask_max = np.max(mask_arr)\n",
    "\n",
    "irates_max = irates_arr.max()\n",
    "log_irates_arr = np.log10(irates_max.value)/np.log10(irates_arr.value) # zero when nan or inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4533a6d",
   "metadata": {},
   "source": [
    "Preparing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811e560-9e1c-4d68-9f4a-d5c1146c7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the random subvolumes we'll consider.\n",
    "rand_train_i, rand_train_j, rand_train_k = np.random.randint(0, 300-subvolume_size, size=(3, nb_train))\n",
    "rand_test_i, rand_test_j, rand_test_k = np.random.randint(0, 300-subvolume_size, size=(3, nb_test))\n",
    "\n",
    "# training\n",
    "training_set = np.zeros((46*nb_train, 3, subvolume_size, subvolume_size, subvolume_size), dtype=np.float32)\n",
    "training_truth = np.zeros((46*nb_train, 1), dtype=np.float32)\n",
    "training_time = np.zeros((46*nb_train), dtype=np.float32)\n",
    "training_nh = np.zeros((46*nb_train), dtype=np.float32)\n",
    "training_irates = np.zeros((46*nb_train), dtype=np.float32)\n",
    "\n",
    "for batch in tqdm(range(nb_train), desc=\"Creating training batches\"):\n",
    "    training_set[batch*46:(batch+1)*46, 0]   =   nsrc_arr[:, rand_train_i[batch]:rand_train_i[batch]+subvolume_size, rand_train_j[batch]:rand_train_j[batch]+subvolume_size, rand_train_k[batch]:rand_train_k[batch]+subvolume_size] / nsrc_max\n",
    "    training_set[batch*46:(batch+1)*46, 1]   =    rho_arr[:, rand_train_i[batch]:rand_train_i[batch]+subvolume_size, rand_train_j[batch]:rand_train_j[batch]+subvolume_size, rand_train_k[batch]:rand_train_k[batch]+subvolume_size] / rho_max\n",
    "    training_set[batch*46:(batch+1)*46, 2]   =   mask_arr[:, rand_train_i[batch]:rand_train_i[batch]+subvolume_size, rand_train_j[batch]:rand_train_j[batch]+subvolume_size, rand_train_k[batch]:rand_train_k[batch]+subvolume_size] / mask_max\n",
    "    training_truth[batch*46:(batch+1)*46,0]  =   xHII_arr[:, (2*rand_train_i[batch]+subvolume_size)//2, (2*rand_train_j[batch]+subvolume_size)//2, (2*rand_train_k[batch]+subvolume_size)//2]\n",
    "    training_nh[batch*46:(batch+1)*46]       =     nh_arr[:, (2*rand_train_i[batch]+subvolume_size)//2, (2*rand_train_j[batch]+subvolume_size)//2, (2*rand_train_k[batch]+subvolume_size)//2]\n",
    "    training_irates[batch*46:(batch+1)*46]   = irates_arr[:, (2*rand_train_i[batch]+subvolume_size)//2, (2*rand_train_j[batch]+subvolume_size)//2, (2*rand_train_k[batch]+subvolume_size)//2]\n",
    "    training_time[batch*46:(batch+1)*46]     = norm_time_arr\n",
    "\n",
    "\n",
    "# testing\n",
    "testing_set = np.zeros((46*nb_train, 3, subvolume_size, subvolume_size, subvolume_size), dtype=np.float32)\n",
    "testing_truth = np.zeros((46*nb_train, 1), dtype=np.float32)\n",
    "testing_time = np.zeros((46*nb_train), dtype=np.float32)\n",
    "testing_nh = np.zeros((46*nb_train), dtype=np.float32)\n",
    "testing_irates = np.zeros((46*nb_train), dtype=np.float32)\n",
    "\n",
    "for batch in tqdm(range(nb_test), desc=\"Creating training batches\"):\n",
    "    testing_set[batch*46:(batch+1)*46, 0]   =   nsrc_arr[:, rand_test_i[batch]:rand_test_i[batch]+subvolume_size, rand_test_j[batch]:rand_test_j[batch]+subvolume_size, rand_test_k[batch]:rand_test_k[batch]+subvolume_size] / nsrc_max\n",
    "    testing_set[batch*46:(batch+1)*46, 1]   =    rho_arr[:, rand_test_i[batch]:rand_test_i[batch]+subvolume_size, rand_test_j[batch]:rand_test_j[batch]+subvolume_size, rand_test_k[batch]:rand_test_k[batch]+subvolume_size] / rho_max\n",
    "    testing_set[batch*46:(batch+1)*46, 2]   =   mask_arr[:, rand_test_i[batch]:rand_test_i[batch]+subvolume_size, rand_test_j[batch]:rand_test_j[batch]+subvolume_size, rand_test_k[batch]:rand_test_k[batch]+subvolume_size] / mask_max\n",
    "    testing_truth[batch*46:(batch+1)*46, 0] =   xHII_arr[:, (2*rand_test_i[batch]+subvolume_size)//2, (2*rand_test_j[batch]+subvolume_size)//2, (2*rand_test_k[batch]+subvolume_size)//2]\n",
    "    testing_nh[batch*46:(batch+1)*46]       =     nh_arr[:, (2*rand_test_i[batch]+subvolume_size)//2, (2*rand_test_j[batch]+subvolume_size)//2, (2*rand_test_k[batch]+subvolume_size)//2]\n",
    "    testing_irates[batch*46:(batch+1)*46]   = irates_arr[:, (2*rand_test_i[batch]+subvolume_size)//2, (2*rand_test_j[batch]+subvolume_size)//2, (2*rand_test_k[batch]+subvolume_size)//2]\n",
    "    testing_time[batch*46:(batch+1)*46]     = norm_time_arr\n",
    "\n",
    "\n",
    "# plotting\n",
    "plot_set = np.zeros((plot_size**2, 3, subvolume_size, subvolume_size, subvolume_size), dtype=np.float32)\n",
    "plot_truth = np.zeros((plot_size**2, 1), dtype=np.float32)\n",
    "\n",
    "# pick the centre of the cube\n",
    "centre = [s//2 for s in xHII_arr.shape][1:]\n",
    "time_plot = 32\n",
    "\n",
    "for j in tqdm(range(centre[1] - plot_size//2, centre[1] + plot_size//2, 1), desc=\"Iterating px for plot\"):\n",
    "    for k in range(centre[2] - plot_size//2, centre[2] + plot_size//2, 1):\n",
    "        batch = (j-centre[1]+plot_size//2)*plot_size + k-centre[2]+plot_size//2\n",
    "        plot_set[batch, 0] = nsrc_arr[time_plot, centre[0]:centre[0]+subvolume_size, j:j+subvolume_size, k:k+subvolume_size] / nsrc_max\n",
    "        plot_set[batch, 1] = rho_arr[time_plot, centre[0]:centre[0]+subvolume_size, j:j+subvolume_size, k:k+subvolume_size] / rho_max\n",
    "        plot_set[batch, 2] = mask_arr[time_plot, centre[0]:centre[0]+subvolume_size, j:j+subvolume_size, k:k+subvolume_size] / mask_max\n",
    "        plot_truth[batch, 0] = xHII_arr[time_plot, (2*centre[0]+subvolume_size)//2, (2*j+subvolume_size)//2, (2*k+subvolume_size)//2]\n",
    "\n",
    "plot_time = np.repeat(norm_time_arr[time_plot], plot_size**2)\n",
    "\n",
    "# convert everything to pytorch tensor\n",
    "training_set    = torch.from_numpy(training_set).requires_grad_(True)\n",
    "training_truth  = torch.from_numpy(training_truth)\n",
    "training_irates = torch.from_numpy(training_irates)\n",
    "training_nh     = torch.from_numpy(training_nh)\n",
    "training_time   = torch.from_numpy(training_time)\n",
    "\n",
    "testing_set     = torch.from_numpy(testing_set)\n",
    "testing_truth   = torch.from_numpy(testing_truth)\n",
    "testing_irates  = torch.from_numpy(testing_irates)\n",
    "testing_nh      = torch.from_numpy(testing_nh)\n",
    "testing_time    = torch.from_numpy(testing_time)\n",
    "\n",
    "plot_set        = torch.from_numpy(plot_set)\n",
    "plot_truth      = torch.from_numpy(plot_truth)\n",
    "plot_time       = torch.from_numpy(plot_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05391fc4-f412-4215-97e9-052e6ef5a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative(file, epoch, predicted, truth, r2, show=False):\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    plt.suptitle(f\"Epoch: {epoch}\")\n",
    "    plt.subplot(131)\n",
    "    pos = plt.imshow(predicted, origin='lower', norm=mpl.colors.Normalize(vmin=0, vmax=1), interpolation='none')\n",
    "    plt.title(\"Predicted\")\n",
    "    fig.colorbar(pos)\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    pos = plt.imshow(truth, origin='lower', norm=mpl.colors.Normalize(vmin=0, vmax=1), interpolation='none')\n",
    "    plt.title(\"Truth\")\n",
    "    fig.colorbar(pos)\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    pos = plt.imshow(predicted - truth, origin='lower', cmap='bwr', norm=mpl.colors.Normalize(vmin=-1, vmax=1), interpolation='none')\n",
    "    plt.title(\"Diff: $R^2$={:.2e}\".format(1-r2))\n",
    "    fig.colorbar(pos)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_data(nsrc, rho, irates):\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(131)\n",
    "    pos = plt.imshow(nsrc, origin='lower', interpolation='none', cmap='bw')\n",
    "    plt.title(\"nsrc\")\n",
    "    fig.colorbar(pos)\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    pos = plt.imshow(rho, origin='lower', interpolation='none', cmap='PiYG')\n",
    "    plt.title(\"rho\")\n",
    "    fig.colorbar(pos)\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    pos = plt.imshow(irates, origin='lower', interpolation='none', cmap='Oranges')\n",
    "    plt.title(\"approx irates\")\n",
    "    fig.colorbar(pos)\n",
    "              \n",
    "    plt.show()\n",
    "    \n",
    "def plot_statistics(file, epoch, prediction, truth, show=False):    \n",
    "    # compute box size\n",
    "    shape = prediction.shape\n",
    "    fullboxsize = 500/0.743 # in Mpc\n",
    "    resolution = fullboxsize/300 # in Mpc\n",
    "    boxsize = shape[0] * resolution\n",
    "\n",
    "    # compute power spectrum\n",
    "    ps1t, ks1t = t2c.power_spectrum_1d(truth,      kbins=15, box_dims=boxsize)\n",
    "    ps1p, ks1p = t2c.power_spectrum_1d(prediction, kbins=15, box_dims=boxsize)\n",
    "    \n",
    "    # compute bubble size\n",
    "    r_mfp1t, dn_mfp1t = t2c.mfp(truth>0.5, boxsize=boxsize, iterations=1000000)\n",
    "    r_mfp1p, dn_mfp1p = t2c.mfp(prediction>0.5, boxsize=boxsize, iterations=1000000)\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize = (10,5))\n",
    "    plt.suptitle(f\"Morphology study of 2D slice for epoch {epoch}\")\n",
    "    \n",
    "    nan_mask_t = np.isnan(ks1t*ps1t) == False\n",
    "    ft = ps1t[nan_mask_t]*ks1t[nan_mask_t]**3/2/np.pi**2\n",
    "    xt = ks1t[nan_mask_t]\n",
    "    tott = np.trapz(ft, xt)\n",
    "    avt = np.trapz(xt * ft / tott, xt)\n",
    "\n",
    "    print(f\"Truth av: {avt}\")\n",
    "    print(xt, ft)\n",
    "    \n",
    "    nan_mask_p = np.isnan(ks1p*ps1p) == False\n",
    "    fp = ps1p[nan_mask_p]*ks1t[nan_mask_p]**3/2/np.pi**2\n",
    "    xp = ks1p[nan_mask_p]\n",
    "    totp = np.trapz(fp, xp)\n",
    "    avp = np.trapz(xp * fp / totp, xp)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.title('Spherically averaged power spectrum')\n",
    "    plt.plot(xt, ft, '-',  color='C0', label=\"Truth: xHII={:.3f}\".format(np.mean(truth)))\n",
    "    plt.plot(xp, fp, '--', color='C0', label=\"Predi: xHII={:.3f}\".format(np.mean(prediction)))\n",
    "    ylim = [min(min(ft), min(fp)), max(max(ft), max(fp))]\n",
    "    plt.plot([avt, avt], ylim, '-', color='black', label=\"Mean truth\")\n",
    "    plt.plot([avp, avp], ylim, '--', color='black', label=\"Mean predi\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('k (Mpc$^{-1}$)')\n",
    "    plt.ylabel('P(k) k$^{3}$/$(2\\pi^2)$')\n",
    "    plt.grid(True, linewidth=.1)\n",
    "    plt.legend()\n",
    "    \n",
    "    nan_mask_t = np.isnan(r_mfp1t*dn_mfp1t) == False\n",
    "    ft = dn_mfp1t[nan_mask_t]\n",
    "    xt = r_mfp1t[nan_mask_t]\n",
    "    \n",
    "    tott = np.trapz(ft, xt)\n",
    "    avt = np.trapz(xt * ft / tott, xt)\n",
    "    bft = np.trapz(ft[xt<=avt], xt[xt<=avt])\n",
    "    aft = np.trapz(ft[xt>=avt], xt[xt>=avt])\n",
    "    \n",
    "    nan_mask_p = np.isnan(r_mfp1p*dn_mfp1p) == False\n",
    "    fp = dn_mfp1p[nan_mask_p]\n",
    "    xp = r_mfp1p[nan_mask_p]\n",
    "    \n",
    "    totp = np.trapz(fp, xp)\n",
    "    avp = np.trapz(xp * fp / totp, xp)\n",
    "    bfp = np.trapz(fp[xp<=avp], xp[xp<=avp])\n",
    "    afp = np.trapz(fp[xp>=avp], xp[xp>=avp])\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Bubble size: Mean free path method')\n",
    "    plt.plot(r_mfp1t, dn_mfp1t, '-',  label=\"Truth: xHII={:.3f}\".format(np.mean(truth)), color='C0')\n",
    "    plt.plot(r_mfp1p, dn_mfp1p, '--', label=\"Predi: xHII={:.3f}\".format(np.mean(prediction)), color='C0')\n",
    "    ylim = [min(min(ft), min(fp)), max(max(ft), max(fp))]\n",
    "    plt.plot([avt, avt], ylim, '-', color='black', label=\"Mean truth\")\n",
    "    plt.plot([avp, avp], ylim, '--', color='black', label=\"Mean predi\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('$R$ (Mpc)')\n",
    "    plt.ylabel('$R\\mathrm{d}P/\\mathrm{d}R$')\n",
    "    plt.grid(True, linewidth=.1)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    labelled_map1t, volumes1t = t2c.fof(truth)\n",
    "    fof_dist1t = t2c.plot_fof_sizes(volumes1t, bins=30, boxsize=boxsize)\n",
    "    labelled_map1p, volumes1p = t2c.fof(prediction)\n",
    "    fof_dist1p = t2c.plot_fof_sizes(volumes1p, bins=30, boxsize=boxsize)\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.title('Friends of friends method')\n",
    "    plt.step(fof_dist1t[0], fof_dist1t[1], '-' , color='C0', label=\"xHII = {:.3f}\".format(np.mean(truth)))\n",
    "    plt.step(fof_dist1p[0], fof_dist1p[1], '--', color='C0', label=\"xHII = {:.3f}\".format(np.mean(prediction)))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(min(fof_dist1t[2], fof_dist1p[2]),1)\n",
    "    plt.xlabel('$V$ (Mpc$^3$)')\n",
    "    plt.ylabel('$V^2\\mathrm{d}P/\\mathrm{d}V$')\n",
    "    plt.grid(True, linewidth=0.1)\n",
    "    plt.legend()\n",
    "    \"\"\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def plot_loss(file, total_loss, data_loss, pinn_loss, validation_loss, learning_rate, show=False):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    ax.plot(total_loss, label=\"Total\")\n",
    "    ax.plot(data_loss, label=\"Data\")\n",
    "    ax.plot(pinn_loss, label=\"Physics\")\n",
    "    ax.plot(validation_loss, label=\"Validation\")\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Losses\")\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(learning_rate, '--', c='gray')\n",
    "    ax2.set_ylabel(\"Learning rate\")\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_random_string(length = 6):\n",
    "    import random, string\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k = length))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    return [ group['lr'] for group in optimizer.param_groups ][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62205d03",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965ea7c-9e8f-44fd-86e0-093ddd2a98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import central_cnn as cnn\n",
    "import r2score as r2s\n",
    "\n",
    "reload(cnn)\n",
    "reload(mpl)\n",
    "reload(r2s)\n",
    "\n",
    "\n",
    "# 1) Define the models\n",
    "model = cnn.CentralCNNV2(3, 1, n_pool, n_features, kernel_size, subvolume_size, n_fcn_layers, fcn_div_factor, maxpool_size, maxpool_stride).to(device)\n",
    "UID = generate_random_string(6) # id to save stuff and avoid overriding plots.\n",
    "print(f\"Unique ID for this run: {UID}\")\n",
    "\n",
    "from torchinfo import summary\n",
    "print(summary(model, [(920, 3, subvolume_size, subvolume_size, subvolume_size), (920, 1)]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4935e2",
   "metadata": {},
   "source": [
    "Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef051a9-379e-475a-94c2-d89edd813d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) loss and opt\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) # it will optimize both gamma and x at the same time\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # default patience: 10 epochs\n",
    "if score_type == 'mse':\n",
    "    criterion = nn.MSELoss() #wmse.WeightedMSELoss().to(device)\n",
    "elif score_type == 'r2':\n",
    "    criterion = r2s.InvertedR2Score()\n",
    "else:\n",
    "    assert False, f\"The score type that you chose doesn't exist: {score_type}\"\n",
    "\n",
    "    \n",
    "    \n",
    "total_losses, data_losses, validation_losses, pinn_losses = [], [], [], []\n",
    "learning_rates = []\n",
    "\n",
    "best_loss = 1e15\n",
    "\n",
    "files_loss, files_slice, files_morph = [], [], []\n",
    "\n",
    "# we do a first evaluation round to \"normalize\" the losses\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pinn_loss, train_data_loss,  = 0,0\n",
    "    for batch in tqdm(range((46*nb_train)//batch_size), position=1, leave=False, disable=False):\n",
    "        # free the optimizer (otherwise it will accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # predict the result\n",
    "        input_train_x = training_set[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "        input_train_t = training_time[batch_size*batch:batch_size*(batch+1)].view(-1, 1).to(device)\n",
    "        truth_train   = training_truth[batch_size*batch:batch_size*(batch+1),0].to(device)\n",
    "        truth_irates  = training_irates[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "        truth_nh      = training_nh[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "\n",
    "        prediction = model(input_train_x, input_train_t).view(-1)\n",
    "        \n",
    "        # get the loss\n",
    "        prediction_cut  = prediction.view(-1,46)[:,cut]\n",
    "        truth_train_cut = truth_train.view(-1,46)[:,cut]\n",
    "        train_data_loss_batch = criterion(prediction_cut, truth_train_cut)\n",
    "        \n",
    "        \n",
    "        # physics with finite difference\n",
    "        time_batch = input_train_t.reshape(-1, 46)*time_max.value\n",
    "        dt = time_batch[:,2:] - time_batch[:,:-2]\n",
    "        xh = prediction.reshape(-1, 46)\n",
    "        xi = xh[:,:-2]\n",
    "        xip1 = xh[:,1:-1]\n",
    "        xip2 = xh[:,2:]\n",
    "        gamma = truth_irates.view(-1, 46)\n",
    "        gammai = gamma[:,:-2]\n",
    "        gammaip1 = gamma[:,1:-1]\n",
    "        gammaip2 = gamma[:,2:]\n",
    "        \n",
    "        xh_truth = truth_train.reshape(-1,46)\n",
    "        xip2_truth = xh_truth[:,2:]\n",
    "        \n",
    "        nh = truth_nh.view(-1, 46)\n",
    "        nhi   = nh[:,:-2]\n",
    "        nhip1 = nh[:,1:-1]\n",
    "        nhip2 = nh[:,2:]\n",
    "        \n",
    "        Di = alpha.value*nhi\n",
    "        Dip1 = alpha.value*nhip1\n",
    "        Dip2 = alpha.value*nhip2\n",
    "        \n",
    "        k1 = (1-xi)*gammai - Di*xi**2\n",
    "        k2 = (1-xi-dt/2*k1)*gammaip1 - Dip1*(xi+dt/2*k1)**2\n",
    "        k3 = (1-xi-dt/2*k2)*gammaip1 - Dip1*(xi+dt/2*k2)**2\n",
    "        k4 = (1-xi-dt*k3  )*gammaip2 - Dip2*(xi+dt*k3  )**2\n",
    "    \n",
    "        xip2_pred = xi + (k1 + 2*k2 + 2*k3 + k4) * dt / 6\n",
    "        xdiff = (torch.clip(torch.abs(xip2_pred), 0, 1) - xi)/dt\n",
    "        physics = xdiff - ( (1-xi)*gammai - Di*xi**2 )\n",
    "        \n",
    "        #compute the loss of the physics, i.e. compute the mean square error\n",
    "        # Note: We force the first value to be 1.\n",
    "        PINNLoss = torch.mean(physics**2) * pinn_multiplication_factor\n",
    "        \n",
    "        # training loss\n",
    "        loss_batch = train_data_loss_batch + PINNLoss\n",
    "                \n",
    "        # save the losses\n",
    "        train_data_loss += train_data_loss_batch.item()\n",
    "        train_pinn_loss += PINNLoss.item()\n",
    "        \n",
    "    # save the initial losses\n",
    "    init_train_data_loss = train_data_loss\n",
    "    init_train_pinn_loss = train_pinn_loss\n",
    "    \n",
    "    \n",
    "    test_data_loss = 0\n",
    "\n",
    "    for batch in tqdm(range((46*nb_test)//batch_size), position=1, leave=False, disable=False):\n",
    "        # predict the result\n",
    "        input_test_x = testing_set[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "        input_test_t = testing_time[batch_size*batch:batch_size*(batch+1)].view(-1, 1).to(device)\n",
    "        truth_test   = testing_truth[batch_size*batch:batch_size*(batch+1),0].to(device)\n",
    "        truth_irates  = testing_irates[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "        truth_nh      = testing_nh[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "\n",
    "        prediction = model(input_test_x, input_test_t).view(-1)\n",
    "\n",
    "        # get the loss\n",
    "        # Note: We force the first value to be one\n",
    "        test_data_loss_batch = criterion(prediction, truth_test) \n",
    "\n",
    "        # save the losses\n",
    "        test_data_loss += train_data_loss_batch.item()\n",
    "\n",
    "    init_test_data_loss = test_data_loss\n",
    "    \n",
    "\n",
    "for epoch in tqdm(range(nb_epoch), desc=\"Iterating epoch\", position=0):\n",
    "    model.train()\n",
    "    train_total_loss, train_pinn_loss, train_data_loss = 0,0,0\n",
    "    \n",
    "    for batch in tqdm(range((46*nb_train)//batch_size), position=1, leave=False, disable=False):\n",
    "        # free the optimizer (otherwise it will accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # predict the result\n",
    "        input_train_x = training_set[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "        input_train_t = training_time[batch_size*batch:batch_size*(batch+1)].view(-1, 1).to(device)\n",
    "        truth_train   = training_truth[batch_size*batch:batch_size*(batch+1),0].to(device)\n",
    "        truth_irates  = training_irates[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "        truth_nh      = training_nh[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "\n",
    "        prediction = model(input_train_x, input_train_t).view(-1)\n",
    "        \n",
    "        # get the loss\n",
    "        prediction_cut  = prediction.view(-1,46)[:,cut]\n",
    "        truth_train_cut = truth_train.view(-1,46)[:,cut]\n",
    "        train_data_loss_batch = criterion(prediction_cut, truth_train_cut) / init_train_data_loss\n",
    "        \n",
    "        # physics with finite difference\n",
    "        time_batch = input_train_t.reshape(-1, 46)*time_max.value\n",
    "        dt = time_batch[:,2:] - time_batch[:,:-2]\n",
    "        xh = prediction.reshape(-1, 46)\n",
    "        xi = xh[:,:-2]\n",
    "        xip1 = xh[:,1:-1]\n",
    "        xip2 = xh[:,2:]\n",
    "        gamma = truth_irates.view(-1, 46)\n",
    "        gammai = gamma[:,:-2]\n",
    "        gammaip1 = gamma[:,1:-1]\n",
    "        gammaip2 = gamma[:,2:]\n",
    "                \n",
    "        nh = truth_nh.view(-1, 46)\n",
    "        nhi   = nh[:,:-2]\n",
    "        nhip1 = nh[:,1:-1]\n",
    "        nhip2 = nh[:,2:]\n",
    "        \n",
    "        Di = alpha.value*nhi\n",
    "        Dip1 = alpha.value*nhip1\n",
    "        Dip2 = alpha.value*nhip2\n",
    "        \n",
    "        k1 = (1-xi)*gammai - Di*xi**2\n",
    "        k2 = (1-xi-dt/2*k1)*gammaip1 - Dip1*(xi+dt/2*k1)**2\n",
    "        k3 = (1-xi-dt/2*k2)*gammaip1 - Dip1*(xi+dt/2*k2)**2\n",
    "        k4 = (1-xi-dt*k3  )*gammaip2 - Dip2*(xi+dt*k3  )**2\n",
    "    \n",
    "        xip2_pred = xi + (k1 + 2*k2 + 2*k3 + k4) * dt / 6\n",
    "        \n",
    "        xdiff = (torch.clip(torch.abs(xip2_pred), 0, 1) - xi)/dt\n",
    "        physics = xdiff - ( (1-xi)*gammai - Di*xi**2 )\n",
    "        \n",
    "\n",
    "        #compute the loss of the physics, i.e. compute the mean square error\n",
    "        # Note: We force the first value to be 1.\n",
    "        PINNLoss = torch.mean(physics**2) / init_train_pinn_loss * pinn_multiplication_factor\n",
    "        \n",
    "        # training loss\n",
    "        loss_batch = train_data_loss_batch + PINNLoss\n",
    "        \n",
    "        # backward\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # save the losses\n",
    "        train_total_loss += loss_batch.item()\n",
    "        train_data_loss += train_data_loss_batch.item()\n",
    "        train_pinn_loss += PINNLoss.item()\n",
    "        \n",
    "    total_losses.append(train_total_loss)\n",
    "    data_losses.append(train_data_loss)\n",
    "    pinn_losses.append(train_pinn_loss)     \n",
    "    \n",
    "    \n",
    "    # adapt learning rate.\n",
    "    learning_rates.append(get_lr(optimizer))\n",
    "    scheduler.step(train_total_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data_loss = 0\n",
    "\n",
    "        for batch in tqdm(range((46*nb_test)//batch_size), position=1, leave=False, disable=False):\n",
    "            # predict the result\n",
    "            input_test_x = testing_set[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "            input_test_t = testing_time[batch_size*batch:batch_size*(batch+1)].view(-1, 1).to(device)\n",
    "            truth_test   = testing_truth[batch_size*batch:batch_size*(batch+1),0].to(device)\n",
    "            truth_irates  = testing_irates[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "            truth_nh      = testing_nh[batch_size*batch:batch_size*(batch+1)].to(device)\n",
    "\n",
    "            prediction = model(input_test_x, input_test_t).view(-1)#.view(truth_train.shape)\n",
    "\n",
    "            # get the loss\n",
    "            # Note: We force the first value to be one\n",
    "            test_data_loss_batch = criterion(prediction, truth_test) / init_test_data_loss\n",
    "\n",
    "            # save the losses\n",
    "            test_data_loss += train_data_loss_batch.item()\n",
    "        \n",
    "        validation_losses.append(test_data_loss)\n",
    "\n",
    "        # save the model if the loss is smaller\n",
    "        if best_loss > test_data_loss:\n",
    "            print(f\"Best new loss: {test_data_loss}\")\n",
    "            best_loss = test_data_loss\n",
    "            torch.save(model.state_dict(), f\"{savepath}C-CNN-V2-model-{UID}.pt\")\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            plot_input_x = plot_set.to(device)\n",
    "            plot_input_t  = plot_time.to(device).view(-1, 1)\n",
    "\n",
    "            prediction = model(plot_input_x, plot_input_t).view(-1)\n",
    "\n",
    "            prediction = prediction.reshape((plot_size, plot_size)).cpu()\n",
    "            truth = plot_truth.reshape((plot_size, plot_size)).cpu()\n",
    "\n",
    "            r2score = criterion(prediction, truth)\n",
    "\n",
    "            prediction = prediction.numpy()\n",
    "            truth = truth.numpy()\n",
    "\n",
    "            shape = plot_input_x.shape\n",
    "            centre = [s//2 for s in shape]\n",
    "            plot_nsrc   = plot_input_x[:,0, centre[2], centre[3], centre[4]].reshape((plot_size, plot_size)).cpu().numpy()\n",
    "            plot_rho    = plot_input_x[:,1, centre[2], centre[3], centre[4]].reshape((plot_size, plot_size)).cpu().numpy()\n",
    "            plot_irates = plot_input_x[:,2, centre[2], centre[3], centre[4]].reshape((plot_size, plot_size)).cpu().numpy()\n",
    "\n",
    "            \n",
    "            file = \"plots/C-CNN-V2-{}_slice_{:08d}.png\".format(UID, epoch+1)\n",
    "            files_slice.append(file)\n",
    "            plot_comparative(file, epoch+1, prediction, truth, r2score, show=show)\n",
    "            file = \"plots/C-CNN-V2-{}_loss_{:08d}.png\".format(UID, epoch+1)\n",
    "            files_loss.append(file)\n",
    "            plot_loss(file, total_losses, data_losses, pinn_losses, validation_losses, learning_rates, show=show)\n",
    "            file = \"plots/C-CNN-V2-{}_morphology_{:08d}.png\".format(UID, epoch+1)\n",
    "            files_morph.append(file)\n",
    "            \n",
    "\n",
    "print(UID)\n",
    "\n",
    "# save: the losses and the best epoch \n",
    "np.savez(f'loss/C-CNN-V2-{UID}.npz', train_total=total_losses, train_data=data_losses, train_pinn=pinn_losses, losses_validation=validation_losses, learning_rates=learning_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "pinn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c891e8ca657637a07b62b2ae950b6fd4cb9a6baed4e3a6a047034aa61abd96f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
